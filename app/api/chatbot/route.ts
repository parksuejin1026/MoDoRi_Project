// ğŸ“ app/api/chatbot/route.ts íŒŒì¼ ë‚´ìš© (ìˆ˜ì •ë³¸)

import OpenAI from 'openai';
import { NextResponse } from 'next/server';
// â­ï¸ 1. lib í´ë”ì—ì„œ í†µí•©ëœ í•™ì¹™ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
import { coreRuleData } from '@/lib/ruleData';

// 1. OpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë™ì¼)
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// 2. ì±—ë´‡ì˜ ì—­í• ì„ ì •ì˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸ (ê°€ì¥ ì¤‘ìš”: ë°ì´í„° ì°¸ê³  ëª…ë ¹ í¬í•¨)
const SYSTEM_PROMPT = `
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ì •í™•í•œ í•™ì¹™ ì „ë¬¸ AI ì±—ë´‡ 'Rule-Look'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ, **ë°˜ë“œì‹œ** ì•„ë˜ì˜ [ì œê³µëœ í•™ì¹™ ì›ë¬¸] ë‚´ìš©ì„ **ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ **í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ê·œì •ì— ì—†ëŠ” ë‚´ìš©ì€ 'ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ì •ë³´ëŠ” ì œê³µëœ í•™ì¹™ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.
ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¹œê·¼í•˜ê³  ëª…í™•í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

---
[ì œê³µëœ í•™ì¹™ ì›ë¬¸]
${coreRuleData}  // â­ï¸â­ï¸ ì—¬ê¸°ì— í†µí•©ëœ ëª¨ë“  í•™ì¹™ ë°ì´í„°ê°€ ì‚½ì…ë©ë‹ˆë‹¤. â­ï¸â­ï¸
---
`;

// 3. POST ìš”ì²­ ì²˜ë¦¬ í•¨ìˆ˜ (ë™ì¼)
export async function POST(req: Request) {
  try {
    const { message } = await req.json();

    const chatCompletion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        { role: 'system', content: SYSTEM_PROMPT }, // ìˆ˜ì •ëœ SYSTEM_PROMPT ì‚¬ìš©
        { role: 'user', content: message },
      ],
      temperature: 0.7,
    });

    const reply = chatCompletion.choices[0]?.message?.content || "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.";
    return NextResponse.json({ reply });

  } catch (error) {
    console.error('OpenAI API í†µì‹  ì˜¤ë¥˜:', error);
    return NextResponse.json(
      { error: 'Internal Server Error' },
      { status: 500 }
    );
  }
}